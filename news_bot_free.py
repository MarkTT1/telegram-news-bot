import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import feedparser
import requests
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.constants import ParseMode
import hashlib
import os
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from deep_translator import GoogleTranslator
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NewsConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤"""
    
    CITIES = {
        'alicante': {
            'name': '–ê–ª–∏–∫–∞–Ω—Ç–µ',
            'channel_id': '@ALCTODAY',  # –ó–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ–π –∫–∞–Ω–∞–ª
            'sources': [
                'https://www.informacion.es/rss/alicante.xml',
                'https://alicanteplaza.es/feed',
                'https://www.alicantehoy.es/feed',
            ],
            'keywords': ['alicante', 'alacant', 'costa blanca']
        },
        'valencia': {
            'name': '–í–∞–ª–µ–Ω—Å–∏—è',
            'channel_id': '@your_valencia_channel',  # –ó–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ–π –∫–∞–Ω–∞–ª
            'sources': [
                'https://www.lasprovincias.es/rss/valencia.xml',
                'https://valenciaplaza.com/feed',
            ],
            'keywords': ['valencia', 'val√®ncia', 'comunitat valenciana']
        },
        'barcelona': {
            'name': '–ë–∞—Ä—Å–µ–ª–æ–Ω–∞',
            'channel_id': '@your_barcelona_channel',  # –ó–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ–π –∫–∞–Ω–∞–ª
            'sources': [
                'https://www.lavanguardia.com/rss/barcelona.xml',
                'https://beteve.cat/feed/',
            ],
            'keywords': ['barcelona', 'catalu√±a', 'catalunya']
        }
    }


class NewsParser:
    """–ü–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def parse_rss(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ RSS –ª–µ–Ω—Ç—ã"""
        try:
            feed = feedparser.parse(url)
            news_items = []
            
            for entry in feed.entries[:10]:  # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –Ω–æ–≤–æ—Å—Ç–µ–π
                news_item = {
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'description': entry.get('summary', entry.get('description', '')),
                    'published': entry.get('published', ''),
                    'image_url': self._extract_image(entry, url),
                    'source': feed.feed.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
                }
                news_items.append(news_item)
            
            return news_items
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS {url}: {e}")
            return []
    
    def _extract_image(self, entry, feed_url: str) -> Optional[str]:
        """–£–õ–£–ß–®–ï–ù–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∑–∞–ø–∏—Å–∏ RSS"""
        
        # –ú–µ—Ç–æ–¥ 1: media:content (—Å—Ç–∞–Ω–¥–∞—Ä—Ç RSS)
        if hasattr(entry, 'media_content'):
            for media in entry.media_content:
                if 'image' in media.get('type', '') or media.get('url', '').lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):
                    url = media.get('url')
                    if url:
                        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (media_content): {url}")
                        return url
        
        # –ú–µ—Ç–æ–¥ 2: media:thumbnail
        if hasattr(entry, 'media_thumbnail'):
            url = entry.media_thumbnail[0].get('url')
            if url:
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (media_thumbnail): {url}")
                return url
        
        # –ú–µ—Ç–æ–¥ 3: enclosure (–¥–ª—è –ø–æ–¥–∫–∞—Å—Ç–æ–≤ –∏ –º–µ–¥–∏–∞)
        if hasattr(entry, 'enclosures'):
            for enclosure in entry.enclosures:
                if 'image' in enclosure.get('type', '') or enclosure.get('href', '').lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):
                    url = enclosure.get('href')
                    if url:
                        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (enclosure): {url}")
                        return url
        
        # –ú–µ—Ç–æ–¥ 4: –ü–∞—Ä—Å–∏–Ω–≥ HTML –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
        if hasattr(entry, 'summary') or hasattr(entry, 'description'):
            html_content = entry.get('summary', entry.get('description', ''))
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # –ò—â–µ–º —Ç–µ–≥ img
            img = soup.find('img')
            if img:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
                for attr in ['src', 'data-src', 'data-lazy-src']:
                    url = img.get(attr)
                    if url:
                        # –î–µ–ª–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π URL –µ—Å–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π
                        if url.startswith('//'):
                            url = 'https:' + url
                        elif url.startswith('/'):
                            from urllib.parse import urlparse
                            parsed = urlparse(feed_url)
                            url = f"{parsed.scheme}://{parsed.netloc}{url}"
                        
                        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (HTML img): {url}")
                        return url
        
        # –ú–µ—Ç–æ–¥ 5: –ü–∞—Ä—Å–∏–Ω–≥ content:encoded (WordPress RSS)
        if hasattr(entry, 'content'):
            for content_item in entry.content:
                soup = BeautifulSoup(content_item.get('value', ''), 'html.parser')
                img = soup.find('img')
                if img and img.get('src'):
                    url = img.get('src')
                    if url.startswith('//'):
                        url = 'https:' + url
                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (content:encoded): {url}")
                    return url
        
        # –ú–µ—Ç–æ–¥ 6: –ü–æ–ø—ã—Ç–∫–∞ —Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–æ–≤–æ—Å—Ç–∏ (–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥)
        # –í–ù–ò–ú–ê–ù–ò–ï: –ú–æ–∂–µ—Ç –∑–∞–º–µ–¥–ª–∏—Ç—å —Ä–∞–±–æ—Ç—É, –≤–∫–ª—é—á–∞–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
         return self._fetch_image_from_article(entry.get('link'))
        logger.debug(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è: {entry.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")
        return None
    
    def _fetch_image_from_article(self, article_url: str) -> Optional[str]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å—Ç–∞—Ç—å–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"""
        try:
            response = self.session.get(article_url, timeout=5)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º Open Graph image
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ OG –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {og_image['content']}")
                return og_image['content']
            
            # –ò—â–µ–º Twitter Card image
            twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
            if twitter_image and twitter_image.get('content'):
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ Twitter –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {twitter_image['content']}")
                return twitter_image['content']
            
            # –ò—â–µ–º –ø–µ—Ä–≤–æ–µ –±–æ–ª—å—à–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Å—Ç–∞—Ç—å–µ
            for img in soup.find_all('img'):
                src = img.get('src', '')
                if src and not any(x in src.lower() for x in ['logo', 'icon', 'avatar', 'sprite']):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
                    width = img.get('width', '0')
                    if width.isdigit() and int(width) >= 300:
                        logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {src}")
                        return src
            
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {article_url}: {e}")
        
        return None
    
    def fetch_all_news(self, sources: List[str]) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_news = []
        for source in sources:
            news = self.parse_rss(source)
            all_news.extend(news)
        return all_news


class NewsFilter:
    """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    # –°–ø–∏—Å–æ–∫ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
    SPAM_KEYWORDS = [
        'clasificados', 'anuncio', 'publicidad', 'sorteo',
        'oferta laboral', 'se busca', 'se alquila', 'se vende'
    ]
    
    def __init__(self, storage_file='published_news.json'):
        self.storage_file = storage_file
        self.published_hashes = self._load_published()
    
    def _load_published(self) -> set:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        if os.path.exists(self.storage_file):
            try:
                with open(self.storage_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('hashes', []))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ published_news: {e}")
        return set()
    
    def _save_published(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            with open(self.storage_file, 'w', encoding='utf-8') as f:
                json.dump({'hashes': list(self.published_hashes)}, f)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è published_news: {e}")
    
    def get_news_hash(self, news: Dict) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ö–µ—à–∞ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        content = f"{news['title']}{news['link']}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def is_duplicate(self, news: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç"""
        news_hash = self.get_news_hash(news)
        return news_hash in self.published_hashes
    
    def is_spam(self, news: Dict) -> bool:
        """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º"""
        text = f"{news['title']} {news['description']}".lower()
        return any(spam_word in text for spam_word in self.SPAM_KEYWORDS)
    
    def mark_as_published(self, news: Dict):
        """–û—Ç–º–µ—Ç–∏—Ç—å –Ω–æ–≤–æ—Å—Ç—å –∫–∞–∫ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—É—é"""
        news_hash = self.get_news_hash(news)
        self.published_hashes.add(news_hash)
        self._save_published()
    
    def filter_news(self, news_list: List[Dict], keywords: List[str]) -> List[Dict]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ –¥—É–±–ª–∏–∫–∞—Ç–∞–º"""
        filtered = []
        for news in news_list:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            if self.is_duplicate(news):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ø–∞–º
            if self.is_spam(news):
                logger.info(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω —Å–ø–∞–º: {news['title']}")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            text = f"{news['title']} {news['description']}".lower()
            if any(keyword.lower() in text for keyword in keywords):
                filtered.append(news)
        
        return filtered


class FreeNewsProcessor:
    """–ë–ï–°–ü–õ–ê–¢–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –±–µ–∑ –ø–ª–∞—Ç–Ω—ã—Ö API"""
    
    def __init__(self):
        self.translator = GoogleTranslator(source='auto', target='ru')
    
    def translate_text(self, text: str) -> str:
        """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Google Translate (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)"""
        try:
            # Google Translate –∏–º–µ–µ—Ç –ª–∏–º–∏—Ç –≤ 5000 —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ —Ä–∞–∑
            if len(text) > 5000:
                text = text[:5000]
            
            translated = self.translator.translate(text)
            return translated
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return text  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –µ—Å–ª–∏ –ø–µ—Ä–µ–≤–æ–¥ –Ω–µ —É–¥–∞–ª—Å—è
    
    def clean_html(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤"""
        soup = BeautifulSoup(text, 'html.parser')
        return soup.get_text(separator=' ', strip=True)
    
    def shorten_text(self, text: str, max_sentences: int = 3) -> str:
        """–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–æ N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = text.replace('!', '.').replace('?', '.').split('.')
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ max_sentences –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        short_text = '. '.join(sentences[:max_sentences])
        if short_text and not short_text.endswith('.'):
            short_text += '.'
        
        return short_text
    
    def generate_hashtags(self, city_name: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ö–µ—à—Ç–µ–≥–æ–≤ –¥–ª—è –≥–æ—Ä–æ–¥–∞"""
        hashtags = [f"#{city_name}", "#–ò—Å–ø–∞–Ω–∏—è"]
        
        city_tags = {
            '–ê–ª–∏–∫–∞–Ω—Ç–µ': ['#–ö–æ—Å—Ç–∞–ë–ª–∞–Ω–∫–∞', '#–ê–ª–∏–∫–∞–Ω—Ç–µ'],
            '–í–∞–ª–µ–Ω—Å–∏—è': ['#–í–∞–ª–µ–Ω—Å–∏—è', '#–ö–æ–º—É–Ω–∏–¥–∞–¥–í–∞–ª–µ–Ω—Å–∏–∞–Ω–∞'],
            '–ë–∞—Ä—Å–µ–ª–æ–Ω–∞': ['#–ë–∞—Ä—Å–µ–ª–æ–Ω–∞', '#–ö–∞—Ç–∞–ª–æ–Ω–∏—è']
        }
        
        if city_name in city_tags:
            hashtags = city_tags[city_name]
        
        return hashtags
    
    async def process_news(self, news: Dict, city_name: str) -> Optional[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ –ë–ï–ó –ø–ª–∞—Ç–Ω–æ–≥–æ AI"""
        try:
            # –û—á–∏—â–∞–µ–º HTML –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
            clean_description = self.clean_html(news['description'])
            
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            translated_title = self.translate_text(news['title'])
            
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏ —Å–æ–∫—Ä–∞—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            translated_desc = self.translate_text(clean_description)
            short_desc = self.shorten_text(translated_desc, max_sentences=3)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–µ—à—Ç–µ–≥–∏
            hashtags = self.generate_hashtags(city_name)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –ø–µ—Ä–µ–≤–æ–¥ –Ω–µ —É–¥–∞–ª—Å—è
            if len(short_desc) < 20:
                logger.info(f"–ü—Ä–æ–ø—É—Å–∫ –Ω–æ–≤–æ—Å—Ç–∏ (—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è): {translated_title}")
                return None
            
            return {
                'title': translated_title,
                'text': short_desc,
                'link': news['link'],
                'image_url': news.get('image_url'),
                'hashtags': hashtags,
                'source': news.get('source', '')
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
            return None


class TelegramPublisher:
    """–ü—É–±–ª–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –≤ Telegram"""
    
    def __init__(self, bot_token: str):
        self.bot = Bot(token=bot_token)
    
    async def publish_news(self, channel_id: str, news: Dict):
        """–ü—É–±–ª–∏–∫–∞—Ü–∏—è –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞
            text = f"<b>{news['title']}</b>\n\n"
            text += f"{news['text']}\n\n"
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ö–µ—à—Ç–µ–≥–∏
            if news.get('hashtags'):
                text += ' '.join(news['hashtags']) + '\n\n'
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
            text += f"üì∞ <a href='{news['link']}'>–ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é</a>"
            
            # –ü—É–±–ª–∏–∫—É–µ–º —Å —Ñ–æ—Ç–æ –∏–ª–∏ –±–µ–∑
            if news.get('image_url'):
                try:
                    logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–æ—Ç–æ: {news['image_url']}")
                    await self.bot.send_photo(
                        chat_id=channel_id,
                        photo=news['image_url'],
                        caption=text,
                        parse_mode=ParseMode.HTML
                    )
                    logger.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –° –§–û–¢–û –≤ {channel_id}")
                except Exception as e:
                    logger.warning(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–æ—Ç–æ: {e}")
                    logger.info(f"–ü—É–±–ª–∏–∫—É–µ–º –ë–ï–ó —Ñ–æ—Ç–æ...")
                    await self.bot.send_message(
                        chat_id=channel_id,
                        text=text,
                        parse_mode=ParseMode.HTML,
                        disable_web_page_preview=False
                    )
            else:
                logger.info(f"–§–æ—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—É–±–ª–∏–∫—É–µ–º —Ç–µ–∫—Å—Ç–æ–º")
                await self.bot.send_message(
                    chat_id=channel_id,
                    text=text,
                    parse_mode=ParseMode.HTML,
                    disable_web_page_preview=False
                )
            
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ {channel_id}: {news['title']}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ Telegram: {e}")
            return False


class NewsBot:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –±–æ—Ç–∞"""
    
    def __init__(self, telegram_token: str):
        self.parser = NewsParser()
        self.filter = NewsFilter()
        self.processor = FreeNewsProcessor()  # –ë–ï–ó –ø–ª–∞—Ç–Ω–æ–≥–æ API!
        self.publisher = TelegramPublisher(telegram_token)
        self.scheduler = AsyncIOScheduler()
    
    async def process_city_news(self, city_key: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –æ–¥–Ω–æ–≥–æ –≥–æ—Ä–æ–¥–∞"""
        city_config = NewsConfig.CITIES[city_key]
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {city_config['name']}")
        
        # 1. –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏
        raw_news = self.parser.fetch_all_news(city_config['sources'])
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(raw_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {city_config['name']}")
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Ñ–æ—Ç–æ
        with_images = sum(1 for n in raw_news if n.get('image_url'))
        logger.info(f"–ò–∑ –Ω–∏—Ö {with_images} –Ω–æ–≤–æ—Å—Ç–µ–π –° –§–û–¢–û")
        
        # 2. –§–∏–ª—å—Ç—Ä—É–µ–º
        filtered_news = self.filter.filter_news(raw_news, city_config['keywords'])
        logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(filtered_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        # 3. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ –ø—É–±–ª–∏–∫—É–µ–º
        published_count = 0
        for news in filtered_news[:5]:  # –ë–µ—Ä—ë–º –º–∞–∫—Å–∏–º—É–º 5 –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Ä–∞–∑
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º (–ø–µ—Ä–µ–≤–æ–¥–∏–º –∏ —Å–æ–∫—Ä–∞—â–∞–µ–º)
            processed_news = await self.processor.process_news(news, city_config['name'])
            
            if processed_news:
                # –ü—É–±–ª–∏–∫—É–µ–º
                success = await self.publisher.publish_news(
                    city_config['channel_id'],
                    processed_news
                )
                
                if success:
                    self.filter.mark_as_published(news)
                    published_count += 1
                    await asyncio.sleep(5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º–∏
        
        logger.info(f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ {published_count} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {city_config['name']}")
    
    async def run_once(self):
        """–û–¥–Ω–æ–∫—Ä–∞—Ç–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Å–µ—Ö –≥–æ—Ä–æ–¥–æ–≤"""
        for city_key in NewsConfig.CITIES.keys():
            await self.process_city_news(city_key)
            await asyncio.sleep(10)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –≥–æ—Ä–æ–¥–∞–º–∏
    
    def start_scheduler(self):
        """–ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞
        self.scheduler.add_job(
            self.run_once,
            'interval',
            hours=2,
            id='news_check'
        )
        
        # –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –≤—Ä–µ–º—è
        # –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ 9:00, 13:00, 17:00, 21:00
        # self.scheduler.add_job(self.run_once, 'cron', hour='9,13,17,21')
        
        self.scheduler.start()
        logger.info("–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–ø—É—â–µ–Ω")
    
    async def run(self):
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞"""
        logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω (–ë–ï–°–ü–õ–ê–¢–ù–ê–Ø –≤–µ—Ä—Å–∏—è –±–µ–∑ AI API)")
        logger.info("–£–õ–£–ß–®–ï–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô –í–ö–õ–Æ–ß–Å–ù")
        
        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ —Å—Ä–∞–∑—É
        await self.run_once()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        self.start_scheduler()
        
        # –î–µ—Ä–∂–∏–º –±–æ—Ç–∞ –∞–∫—Ç–∏–≤–Ω—ã–º
        try:
            while True:
                await asyncio.sleep(3600)
        except KeyboardInterrupt:
            logger.info("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


async def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    import os
    # –ß–∏—Ç–∞–µ–º —Ç–æ–∫–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è Railway
    TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN', 'YOUR_TELEGRAM_BOT_TOKEN')
    
    if TELEGRAM_BOT_TOKEN == "YOUR_TELEGRAM_BOT_TOKEN":
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï! –ù–µ –∑–∞–±—É–¥—å –∑–∞–º–µ–Ω–∏—Ç—å TELEGRAM_BOT_TOKEN!")
        print("üìñ –ü–æ–ª—É—á–∏ —Ç–æ–∫–µ–Ω —É @BotFather –≤ Telegram")
        return
    
    bot = NewsBot(TELEGRAM_BOT_TOKEN)
    await bot.run()


if __name__ == "__main__":
    asyncio.run(main())
