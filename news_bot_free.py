import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import feedparser
import requests
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.constants import ParseMode
import hashlib
import os
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from deep_translator import GoogleTranslator

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NewsConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤"""
    
    CITIES = {
        'alicante': {
            'name': '–ê–ª–∏–∫–∞–Ω—Ç–µ',
            'channel_id': '@ALCTODAY',  # –ó–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ–π –∫–∞–Ω–∞–ª
            'sources': [
                'https://www.informacion.es/rss/alicante.xml',
                'https://alicanteplaza.es/feed',
                'https://www.alicantehoy.es/feed',
            ],
            'keywords': ['alicante', 'alacant', 'costa blanca']
        },
        'valencia': {
            'name': '–í–∞–ª–µ–Ω—Å–∏—è',
            'channel_id': '@your_valencia_channel',  # –ó–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ–π –∫–∞–Ω–∞–ª
            'sources': [
                'https://www.lasprovincias.es/rss/valencia.xml',
                'https://valenciaplaza.com/feed',
            ],
            'keywords': ['valencia', 'val√®ncia', 'comunitat valenciana']
        },
        'barcelona': {
            'name': '–ë–∞—Ä—Å–µ–ª–æ–Ω–∞',
            'channel_id': '@your_barcelona_channel',  # –ó–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ–π –∫–∞–Ω–∞–ª
            'sources': [
                'https://www.lavanguardia.com/rss/barcelona.xml',
                'https://beteve.cat/feed/',
            ],
            'keywords': ['barcelona', 'catalu√±a', 'catalunya']
        }
    }


class NewsParser:
    """–ü–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def parse_rss(self, url: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ RSS –ª–µ–Ω—Ç—ã"""
        try:
            feed = feedparser.parse(url)
            news_items = []
            
            for entry in feed.entries[:10]:  # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –Ω–æ–≤–æ—Å—Ç–µ–π
                news_item = {
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'description': entry.get('summary', entry.get('description', '')),
                    'published': entry.get('published', ''),
                    'image_url': self._extract_image(entry),
                    'source': feed.feed.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫')
                }
                news_items.append(news_item)
            
            return news_items
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS {url}: {e}")
            return []
    
    def _extract_image(self, entry) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∑–∞–ø–∏—Å–∏ RSS"""
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª—è—Ö
        if hasattr(entry, 'media_content'):
            for media in entry.media_content:
                if 'image' in media.get('type', ''):
                    return media.get('url')
        
        if hasattr(entry, 'media_thumbnail'):
            return entry.media_thumbnail[0].get('url')
        
        # –ò—â–µ–º –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
        if hasattr(entry, 'summary'):
            soup = BeautifulSoup(entry.summary, 'html.parser')
            img = soup.find('img')
            if img and img.get('src'):
                return img['src']
        
        return None
    
    def fetch_all_news(self, sources: List[str]) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_news = []
        for source in sources:
            news = self.parse_rss(source)
            all_news.extend(news)
        return all_news


class NewsFilter:
    """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    # –°–ø–∏—Å–æ–∫ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
    SPAM_KEYWORDS = [
        'clasificados', 'anuncio', 'publicidad', 'sorteo',
        'oferta laboral', 'se busca', 'se alquila', 'se vende'
    ]
    
    def __init__(self, storage_file='published_news.json'):
        self.storage_file = storage_file
        self.published_hashes = self._load_published()
    
    def _load_published(self) -> set:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —É–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        if os.path.exists(self.storage_file):
            try:
                with open(self.storage_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return set(data.get('hashes', []))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ published_news: {e}")
        return set()
    
    def _save_published(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        try:
            with open(self.storage_file, 'w', encoding='utf-8') as f:
                json.dump({'hashes': list(self.published_hashes)}, f)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è published_news: {e}")
    
    def get_news_hash(self, news: Dict) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ö–µ—à–∞ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        content = f"{news['title']}{news['link']}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def is_duplicate(self, news: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç"""
        news_hash = self.get_news_hash(news)
        return news_hash in self.published_hashes
    
    def is_spam(self, news: Dict) -> bool:
        """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∞–º"""
        text = f"{news['title']} {news['description']}".lower()
        return any(spam_word in text for spam_word in self.SPAM_KEYWORDS)
    
    def mark_as_published(self, news: Dict):
        """–û—Ç–º–µ—Ç–∏—Ç—å –Ω–æ–≤–æ—Å—Ç—å –∫–∞–∫ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—É—é"""
        news_hash = self.get_news_hash(news)
        self.published_hashes.add(news_hash)
        self._save_published()
    
    def filter_news(self, news_list: List[Dict], keywords: List[str]) -> List[Dict]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ –¥—É–±–ª–∏–∫–∞—Ç–∞–º"""
        filtered = []
        for news in news_list:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            if self.is_duplicate(news):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ø–∞–º
            if self.is_spam(news):
                logger.info(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω —Å–ø–∞–º: {news['title']}")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            text = f"{news['title']} {news['description']}".lower()
            if any(keyword.lower() in text for keyword in keywords):
                filtered.append(news)
        
        return filtered


class FreeNewsProcessor:
    """–ë–ï–°–ü–õ–ê–¢–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –±–µ–∑ –ø–ª–∞—Ç–Ω—ã—Ö API"""
    
    def __init__(self):
        self.translator = GoogleTranslator(source='auto', target='ru')
    
    def translate_text(self, text: str) -> str:
        """–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Google Translate (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)"""
        try:
            # Google Translate –∏–º–µ–µ—Ç –ª–∏–º–∏—Ç –≤ 5000 —Å–∏–º–≤–æ–ª–æ–≤ –∑–∞ —Ä–∞–∑
            if len(text) > 5000:
                text = text[:5000]
            
            translated = self.translator.translate(text)
            return translated
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
            return text  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –µ—Å–ª–∏ –ø–µ—Ä–µ–≤–æ–¥ –Ω–µ —É–¥–∞–ª—Å—è
    
    def clean_html(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤"""
        soup = BeautifulSoup(text, 'html.parser')
        return soup.get_text(separator=' ', strip=True)
    
    def shorten_text(self, text: str, max_sentences: int = 3) -> str:
        """–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–æ N –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = text.replace('!', '.').replace('?', '.').split('.')
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ max_sentences –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        short_text = '. '.join(sentences[:max_sentences])
        if short_text and not short_text.endswith('.'):
            short_text += '.'
        
        return short_text
    
    def generate_hashtags(self, city_name: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ö–µ—à—Ç–µ–≥–æ–≤ –¥–ª—è –≥–æ—Ä–æ–¥–∞"""
        hashtags = [f"#{city_name}", "#–ò—Å–ø–∞–Ω–∏—è"]
        
        city_tags = {
            '–ê–ª–∏–∫–∞–Ω—Ç–µ': ['#–ö–æ—Å—Ç–∞–ë–ª–∞–Ω–∫–∞', '#–ê–ª–∏–∫–∞–Ω—Ç–µ'],
            '–í–∞–ª–µ–Ω—Å–∏—è': ['#–í–∞–ª–µ–Ω—Å–∏—è', '#–ö–æ–º—É–Ω–∏–¥–∞–¥–í–∞–ª–µ–Ω—Å–∏–∞–Ω–∞'],
            '–ë–∞—Ä—Å–µ–ª–æ–Ω–∞': ['#–ë–∞—Ä—Å–µ–ª–æ–Ω–∞', '#–ö–∞—Ç–∞–ª–æ–Ω–∏—è']
        }
        
        if city_name in city_tags:
            hashtags = city_tags[city_name]
        
        return hashtags
    
    async def process_news(self, news: Dict, city_name: str) -> Optional[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏ –ë–ï–ó –ø–ª–∞—Ç–Ω–æ–≥–æ AI"""
        try:
            # –û—á–∏—â–∞–µ–º HTML –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
            clean_description = self.clean_html(news['description'])
            
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            translated_title = self.translate_text(news['title'])
            
            # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∏ —Å–æ–∫—Ä–∞—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            translated_desc = self.translate_text(clean_description)
            short_desc = self.shorten_text(translated_desc, max_sentences=3)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ö–µ—à—Ç–µ–≥–∏
            hashtags = self.generate_hashtags(city_name)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏–ª–∏ –ø–µ—Ä–µ–≤–æ–¥ –Ω–µ —É–¥–∞–ª—Å—è
            if len(short_desc) < 20:
                logger.info(f"–ü—Ä–æ–ø—É—Å–∫ –Ω–æ–≤–æ—Å—Ç–∏ (—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è): {translated_title}")
                return None
            
            return {
                'title': translated_title,
                'text': short_desc,
                'link': news['link'],
                'image_url': news.get('image_url'),
                'hashtags': hashtags,
                'source': news.get('source', '')
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
            return None


class TelegramPublisher:
    """–ü—É–±–ª–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –≤ Telegram"""
    
    def __init__(self, bot_token: str):
        self.bot = Bot(token=bot_token)
    
    async def publish_news(self, channel_id: str, news: Dict):
        """–ü—É–±–ª–∏–∫–∞—Ü–∏—è –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞
            text = f"<b>{news['title']}</b>\n\n"
            text += f"{news['text']}\n\n"
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ö–µ—à—Ç–µ–≥–∏
            if news.get('hashtags'):
                text += ' '.join(news['hashtags']) + '\n\n'
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
            text += f"üì∞ <a href='{news['link']}'>–ß–∏—Ç–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é</a>"
            
            # –ü—É–±–ª–∏–∫—É–µ–º —Å —Ñ–æ—Ç–æ –∏–ª–∏ –±–µ–∑
            if news.get('image_url'):
                try:
                    await self.bot.send_photo(
                        chat_id=channel_id,
                        photo=news['image_url'],
                        caption=text,
                        parse_mode=ParseMode.HTML
                    )
                    logger.info(f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ —Å —Ñ–æ—Ç–æ –≤ {channel_id}")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–æ—Ç–æ, –ø—É–±–ª–∏–∫—É–µ–º —Ç–µ–∫—Å—Ç–æ–º: {e}")
                    await self.bot.send_message(
                        chat_id=channel_id,
                        text=text,
                        parse_mode=ParseMode.HTML,
                        disable_web_page_preview=False
                    )
            else:
                await self.bot.send_message(
                    chat_id=channel_id,
                    text=text,
                    parse_mode=ParseMode.HTML,
                    disable_web_page_preview=False
                )
            
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ {channel_id}: {news['title']}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ Telegram: {e}")
            return False


class NewsBot:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –±–æ—Ç–∞"""
    
    def __init__(self, telegram_token: str):
        self.parser = NewsParser()
        self.filter = NewsFilter()
        self.processor = FreeNewsProcessor()  # –ë–ï–ó –ø–ª–∞—Ç–Ω–æ–≥–æ API!
        self.publisher = TelegramPublisher(telegram_token)
        self.scheduler = AsyncIOScheduler()
    
    async def process_city_news(self, city_key: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –æ–¥–Ω–æ–≥–æ –≥–æ—Ä–æ–¥–∞"""
        city_config = NewsConfig.CITIES[city_key]
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {city_config['name']}")
        
        # 1. –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏
        raw_news = self.parser.fetch_all_news(city_config['sources'])
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(raw_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {city_config['name']}")
        
        # 2. –§–∏–ª—å—Ç—Ä—É–µ–º
        filtered_news = self.filter.filter_news(raw_news, city_config['keywords'])
        logger.info(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(filtered_news)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        # 3. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ –ø—É–±–ª–∏–∫—É–µ–º
        published_count = 0
        for news in filtered_news[:5]:  # –ë–µ—Ä—ë–º –º–∞–∫—Å–∏–º—É–º 5 –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Ä–∞–∑
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º (–ø–µ—Ä–µ–≤–æ–¥–∏–º –∏ —Å–æ–∫—Ä–∞—â–∞–µ–º)
            processed_news = await self.processor.process_news(news, city_config['name'])
            
            if processed_news:
                # –ü—É–±–ª–∏–∫—É–µ–º
                success = await self.publisher.publish_news(
                    city_config['channel_id'],
                    processed_news
                )
                
                if success:
                    self.filter.mark_as_published(news)
                    published_count += 1
                    await asyncio.sleep(5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º–∏
        
        logger.info(f"–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ {published_count} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {city_config['name']}")
    
    async def run_once(self):
        """–û–¥–Ω–æ–∫—Ä–∞—Ç–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Å–µ—Ö –≥–æ—Ä–æ–¥–æ–≤"""
        for city_key in NewsConfig.CITIES.keys():
            await self.process_city_news(city_key)
            await asyncio.sleep(10)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –≥–æ—Ä–æ–¥–∞–º–∏
    
    def start_scheduler(self):
        """–ó–∞–ø—É—Å–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞
        self.scheduler.add_job(
            self.run_once,
            'interval',
            hours=1,
            id='news_check'
        )
        
        # –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –≤—Ä–µ–º—è
        # –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ 9:00, 13:00, 17:00, 21:00
        # self.scheduler.add_job(self.run_once, 'cron', hour='9,13,17,21')
        
        self.scheduler.start()
        logger.info("–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–ø—É—â–µ–Ω")
    
    async def run(self):
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞"""
        logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω (–ë–ï–°–ü–õ–ê–¢–ù–ê–Ø –≤–µ—Ä—Å–∏—è –±–µ–∑ AI API)")
        
        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ —Å—Ä–∞–∑—É
        await self.run_once()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        self.start_scheduler()
        
        # –î–µ—Ä–∂–∏–º –±–æ—Ç–∞ –∞–∫—Ç–∏–≤–Ω—ã–º
        try:
            while True:
                await asyncio.sleep(3600)
        except KeyboardInterrupt:
            logger.info("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")


async def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    # –í–ê–ñ–ù–û: –ó–∞–º–µ–Ω–∏ —Ç–æ–∫–µ–Ω –Ω–∞ —Å–≤–æ–π!
    TELEGRAM_BOT_TOKEN = "8413304400:AAE2ob9NPe4UJiT9j0LuHqzuUEKQebuLjDI"  # –¢–æ–∫–µ–Ω –æ—Ç @BotFather
    
    if TELEGRAM_BOT_TOKEN == "YOUR_TELEGRAM_BOT_TOKEN":
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï! –ù–µ –∑–∞–±—É–¥—å –∑–∞–º–µ–Ω–∏—Ç—å TELEGRAM_BOT_TOKEN!")
        print("üìñ –ü–æ–ª—É—á–∏ —Ç–æ–∫–µ–Ω —É @BotFather –≤ Telegram")
        return
    
    bot = NewsBot(TELEGRAM_BOT_TOKEN)
    await bot.run()


if __name__ == "__main__":
    asyncio.run(main())
